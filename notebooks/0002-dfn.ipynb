{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import lightning\n",
    "\n",
    "from retnet import GPTR, GPTRConfig, GPTRClassifier\n",
    "from lra import ListOps, IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c26ce2958b4b6c9dc1bb4d3eab113e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.31k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e46b2541474dae9074528bd2f62b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5dff2c511744aab490e1049a62675b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3624951ebd14e74989f4e2dd455b3a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca55493d40004b75a6fbba5ac4f98ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e4c07d99b9d47f883d5020fef7bb678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d37ff37e2894bb88b5dc088fe78c6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39233af032c4b40977b1b5c12ea0454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a0d10b0bb44d9b8f5bf5277864badd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7039f464144490f82f8f4ea8e55a319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ecbb4b315e64c979285bb0e0e55da6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f867d50feeb418b97ec00b72cfb5e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf021fc4a6b4f7ca403122e87c63660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB char level | min_freq 15 | vocab size 134\n"
     ]
    }
   ],
   "source": [
    "dataset = IMDB(\"imdb\")\n",
    "dataset.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dfnaiff/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 23 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = dataset.train_dataloader(batch_size=32, num_workers=23)\n",
    "valid_dataloader = dataset.val_dataloader(batch_size=32, num_workers=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dfnaiff/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 23 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([batch[0].shape[1] for batch in train_dataloader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[56,  7, 16,  ...,  0,  0,  0],\n",
       "         [55, 11,  4,  ...,  0,  0,  0],\n",
       "         [29,  3, 11,  ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [49,  4, 10,  ...,  0,  0,  0],\n",
       "         [36,  3, 22,  ...,  0,  0,  0],\n",
       "         [70, 29,  3,  ...,  0,  0,  0]]),\n",
       " tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "         1, 1, 0, 1, 1, 1, 1, 1]),\n",
       " {'lengths': tensor([1163,  866,  523, 1641,  658, 1319,  738, 1191,  610,  578,  784,  676,\n",
       "           206,  597,  766,  871,  579,  216, 4096,  668,  806, 2132,  687,  795,\n",
       "          1420, 1340, 1015,  635,  262,  800, 1010, 3667])})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTRConfig(vocab_size=dataset.n_tokens,\n",
    "                    context_window=None,\n",
    "                    nclasses=2,\n",
    "                    embedding_dim=32,\n",
    "                    nheads=2,\n",
    "                    nlayers=2,\n",
    "                    nhidden=32*4,\n",
    "                    )\n",
    "model = GPTRClassifier(config, has_wg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMClassifier(lightning.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.warmup_steps = 1000*8\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        x, y, args = batch\n",
    "        lengths = args['lengths']\n",
    "        logits = self.model(x, lengths)\n",
    "        loss = torch.nn.CrossEntropyLoss()(logits.logits, batch[1])\n",
    "        acc = (torch.argmax(logits.logits, axis=-1) == batch[1]).float().mean()\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, args = batch\n",
    "        logits = self.model.forward(x, args['lengths'])\n",
    "        loss = torch.nn.CrossEntropyLoss()(logits.logits, batch[1])\n",
    "        acc = (torch.argmax(logits.logits, axis=-1) == batch[1]).float().mean()\n",
    "        self.log(\"valid_loss\", loss, prog_bar=True)\n",
    "        self.log(\"valid_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def create_optimizer(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=0.05, weight_decay=0.1)\n",
    "            \n",
    "    def lr_warmup_config(self):\n",
    "        def warmup(step):\n",
    "            \"\"\"\n",
    "            This method will be called for ceil(warmup_batches/accum_grad_batches) times,\n",
    "            warmup_steps has been adjusted accordingly\n",
    "            \"\"\"\n",
    "            if self.warmup_steps <= 0:\n",
    "                factor = 1\n",
    "            else:\n",
    "                factor = min(step / self.warmup_steps, 1)\n",
    "            return factor\n",
    "\n",
    "        opt1 = self.create_optimizer()\n",
    "        return {\n",
    "            'frequency': 1,\n",
    "            'optimizer': opt1,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': torch.optim.lr_scheduler.LambdaLR(opt1, warmup),\n",
    "                'interval': 'step',\n",
    "                'frequency': 1,\n",
    "                'name': 'lr/warmup'\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return (\n",
    "            self.lr_warmup_config(),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = LLMClassifier(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "***** 2\n",
      "tensor(-1.) tensor(-1.0000) tensor(1.) tensor(1.)\n",
      "tensor(False)\n",
      "here\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "***** 2\n",
      "tensor(-1.) tensor(-1.0000) tensor(1.) tensor(1.)\n",
      "tensor(False)\n",
      "here\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "---\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "***** 2\n",
      "tensor(-1.) tensor(-1.0000) tensor(1.) tensor(1.)\n",
      "tensor(True)\n",
      "here\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "***** 2\n",
      "tensor(-1.) tensor(-1.0000) tensor(1.) tensor(1.)\n",
      "tensor(True)\n",
      "here\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "         ...,\n",
       "         [-0.2478, -1.1155, -0.0539,  ...,  0.3282,  0.3801,  1.0476],\n",
       "         [-0.2478, -1.1155, -0.0539,  ...,  0.3282,  0.3801,  1.0476],\n",
       "         [-0.2478, -1.1155, -0.0539,  ...,  0.3282,  0.3801,  1.0476]],\n",
       "\n",
       "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "         ...,\n",
       "         [-0.2478, -1.1155, -0.0539,  ...,  0.3282,  0.3801,  1.0476],\n",
       "         [-0.2478, -1.1155, -0.0539,  ...,  0.3282,  0.3801,  1.0476],\n",
       "         [-0.2478, -1.1155, -0.0539,  ...,  0.3282,  0.3801,  1.0476]],\n",
       "\n",
       "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "         ...,\n",
       "         [-0.2478, -1.1155, -0.0539,  ...,  0.3282,  0.3801,  1.0476],\n",
       "         [-0.2478, -1.1155, -0.0539,  ...,  0.3282,  0.3801,  1.0476],\n",
       "         [-0.2478, -1.1155, -0.0539,  ...,  0.3282,  0.3801,  1.0476]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "         ...,\n",
       "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       "\n",
       "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "         ...,\n",
       "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       "\n",
       "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "         ...,\n",
       "         [-0.2478, -1.1155, -0.0539,  ...,  0.3282,  0.3801,  1.0476],\n",
       "         [-0.2478, -1.1155, -0.0539,  ...,  0.3282,  0.3801,  1.0476],\n",
       "         [-0.2478, -1.1155, -0.0539,  ...,  0.3282,  0.3801,  1.0476]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.model.model.decode(batch[0][:, :2500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(nan, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "module.training_step(batch, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7051,  2.2498, -0.1717,  ...,  0.5472, -1.3477, -1.2243],\n",
       "         [-1.1632,  0.0670, -1.1302,  ..., -1.9171, -1.2850, -1.6140],\n",
       "         [-0.9012,  1.3104, -1.3360,  ..., -0.0637, -1.5882,  0.7112],\n",
       "         ...,\n",
       "         [-0.2015,  0.0227,  0.4610,  ..., -0.4996,  0.2608, -0.0224],\n",
       "         [-0.2015,  0.0227,  0.4610,  ..., -0.4996,  0.2608, -0.0224],\n",
       "         [-0.2015,  0.0227,  0.4610,  ..., -0.4996,  0.2608, -0.0224]],\n",
       "\n",
       "        [[-0.3555, -0.5534, -0.7999,  ...,  0.2185, -1.9083,  0.2078],\n",
       "         [-1.5849, -0.1693,  0.2019,  ...,  0.1344,  0.4728, -0.4233],\n",
       "         [ 1.4271,  0.8038,  1.0006,  ..., -1.0958, -1.3964,  0.9559],\n",
       "         ...,\n",
       "         [-0.2015,  0.0226,  0.4611,  ..., -0.4997,  0.2608, -0.0224],\n",
       "         [-0.2015,  0.0226,  0.4611,  ..., -0.4997,  0.2608, -0.0224],\n",
       "         [-0.2015,  0.0226,  0.4611,  ..., -0.4997,  0.2608, -0.0224]],\n",
       "\n",
       "        [[ 0.7051,  2.2498, -0.1717,  ...,  0.5472, -1.3477, -1.2243],\n",
       "         [-1.1632,  0.0670, -1.1302,  ..., -1.9171, -1.2850, -1.6140],\n",
       "         [-1.3876, -1.2808,  1.5593,  ..., -1.5629, -0.2445, -0.3993],\n",
       "         ...,\n",
       "         [-0.0078, -0.0056, -0.2076,  ...,  0.5805,  0.5550,  2.0733],\n",
       "         [-1.4842,  2.1761, -0.2357,  ..., -1.6406, -0.4996, -0.8161],\n",
       "         [-0.7207, -1.2293,  0.9446,  ..., -2.2549, -0.4611, -0.4394]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.7051,  2.2498, -0.1717,  ...,  0.5472, -1.3477, -1.2243],\n",
       "         [-1.1632,  0.0670, -1.1302,  ..., -1.9171, -1.2850, -1.6140],\n",
       "         [ 0.3270, -0.0074,  0.0780,  ...,  0.2922, -0.2016, -1.9754],\n",
       "         ...,\n",
       "         [-0.2015,  0.0226,  0.4611,  ..., -0.4997,  0.2608, -0.0224],\n",
       "         [-0.2015,  0.0226,  0.4611,  ..., -0.4997,  0.2608, -0.0224],\n",
       "         [-0.2015,  0.0226,  0.4611,  ..., -0.4997,  0.2608, -0.0224]],\n",
       "\n",
       "        [[ 0.7051,  2.2498, -0.1717,  ...,  0.5472, -1.3477, -1.2243],\n",
       "         [-1.1632,  0.0670, -1.1302,  ..., -1.9171, -1.2850, -1.6140],\n",
       "         [ 2.1411,  0.7715,  0.1890,  ...,  0.2062,  2.1614,  1.3568],\n",
       "         ...,\n",
       "         [-0.2015,  0.0226,  0.4611,  ..., -0.4997,  0.2608, -0.0224],\n",
       "         [-0.2015,  0.0226,  0.4611,  ..., -0.4997,  0.2608, -0.0224],\n",
       "         [-0.2015,  0.0226,  0.4611,  ..., -0.4997,  0.2608, -0.0224]],\n",
       "\n",
       "        [[-0.3555, -0.5534, -0.7999,  ...,  0.2185, -1.9083,  0.2078],\n",
       "         [ 0.5744,  0.2440, -1.6746,  ..., -0.5157, -0.0120,  1.6896],\n",
       "         [ 0.6275,  0.1901, -1.7478,  ..., -0.4155,  0.1500,  1.4608],\n",
       "         ...,\n",
       "         [-0.2015,  0.0226,  0.4611,  ..., -0.4997,  0.2608, -0.0224],\n",
       "         [-0.2015,  0.0226,  0.4611,  ..., -0.4997,  0.2608, -0.0224],\n",
       "         [-0.2015,  0.0226,  0.4611,  ..., -0.4997,  0.2608, -0.0224]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.model.model.decode(batch[0][:, :2500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lcadame/miniconda3/envs/ddpm_env/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "/home/lcadame/miniconda3/envs/ddpm_env/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/lcadame/miniconda3/envs/ddpm_env/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py:375: Found unsupported keys in the optimizer configuration: {'frequency'}\n",
      "\n",
      "  | Name  | Type           | Params\n",
      "-----------------------------------------\n",
      "0 | model | GPTRClassifier | 1.2 M \n",
      "-----------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.892     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eaa1526a8584b229de3883e75a8a1dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.80 GiB. GPU 0 has a total capacty of 15.77 GiB of which 333.12 MiB is free. Including non-PyTorch memory, this process has 15.44 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 837.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/lcadame/repos/LRATorch/lra-benchmarks-retnet/0002-dfn.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bexxon-v100/home/lcadame/repos/LRATorch/lra-benchmarks-retnet/0002-dfn.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m lightning\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, accumulate_grad_batches\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bexxon-v100/home/lcadame/repos/LRATorch/lra-benchmarks-retnet/0002-dfn.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model\u001b[39m=\u001b[39;49mmodule, train_dataloaders\u001b[39m=\u001b[39;49mtrain_dataloader, val_dataloaders\u001b[39m=\u001b[39;49mvalid_dataloader)\n",
      "File \u001b[0;32m~/miniconda3/envs/ddpm_env/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    545\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    546\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ddpm_env/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/ddpm_env/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    582\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ddpm_env/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:989\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    986\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    991\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    994\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ddpm_env/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1035\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1034\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1035\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1036\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected state \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ddpm_env/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 202\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance()\n\u001b[1;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ddpm_env/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:359\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    358\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[0;32m~/miniconda3/envs/ddpm_env/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:136\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[1;32m    135\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(data_fetcher)\n\u001b[1;32m    137\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    138\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ddpm_env/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:240\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    239\u001b[0m         \u001b[39m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mautomatic_optimization\u001b[39m.\u001b[39;49mrun(trainer\u001b[39m.\u001b[39;49moptimizers[\u001b[39m0\u001b[39;49m], batch_idx, kwargs)\n\u001b[1;32m    241\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_optimization\u001b[39m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ddpm_env/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:180\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    169\u001b[0m     \u001b[39m# when the strategy handles accumulation, we want to always call the optimizer step\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mhandles_gradient_accumulation\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[39m# -------------------\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     \u001b[39m# automatic_optimization=True: perform ddp sync only when performing optimizer_step\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     \u001b[39mwith\u001b[39;00m _block_parallel_sync_behavior(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy, block\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 180\u001b[0m         closure()\n\u001b[1;32m    182\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step(batch_idx, closure)\n",
      "File \u001b[0;32m~/miniconda3/envs/ddpm_env/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:140\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclosure(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
      "File \u001b[0;32m~/miniconda3/envs/ddpm_env/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ddpm_env/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:126\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39menable_grad()\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosure\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 126\u001b[0m     step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_fn()\n\u001b[1;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarning_cache\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ddpm_env/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:315\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m trainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\n\u001b[1;32m    314\u001b[0m \u001b[39m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m training_step_output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mtraining_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    316\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mpost_training_step()  \u001b[39m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_result_cls\u001b[39m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[39m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[0;32m~/miniconda3/envs/ddpm_env/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    311\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/ddpm_env/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:382\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module:\n\u001b[1;32m    381\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_redirection(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39m\"\u001b[39m\u001b[39mtraining_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 382\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module\u001b[39m.\u001b[39;49mtraining_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/lcadame/repos/LRATorch/lra-benchmarks-retnet/0002-dfn.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bexxon-v100/home/lcadame/repos/LRATorch/lra-benchmarks-retnet/0002-dfn.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m x, y, args \u001b[39m=\u001b[39m batch\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bexxon-v100/home/lcadame/repos/LRATorch/lra-benchmarks-retnet/0002-dfn.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m lengths \u001b[39m=\u001b[39m args[\u001b[39m'\u001b[39m\u001b[39mlengths\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bexxon-v100/home/lcadame/repos/LRATorch/lra-benchmarks-retnet/0002-dfn.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x, lengths)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bexxon-v100/home/lcadame/repos/LRATorch/lra-benchmarks-retnet/0002-dfn.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mCrossEntropyLoss()(logits\u001b[39m.\u001b[39mlogits, batch[\u001b[39m1\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bexxon-v100/home/lcadame/repos/LRATorch/lra-benchmarks-retnet/0002-dfn.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m acc \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39margmax(logits\u001b[39m.\u001b[39mlogits, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m batch[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/miniconda3/envs/ddpm_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ddpm_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/LRATorch/lra-benchmarks-retnet/retnet.py:378\u001b[0m, in \u001b[0;36mGPTRClassifier.forward\u001b[0;34m(self, input_ids, lengths)\u001b[0m\n\u001b[1;32m    376\u001b[0m index_seq \u001b[39m=\u001b[39m lengths\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m    377\u001b[0m index_batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[0;32m--> 378\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mdecode(x)\n\u001b[1;32m    379\u001b[0m x \u001b[39m=\u001b[39m x[index_batch, index_seq, :]\n\u001b[1;32m    380\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(x)\n",
      "File \u001b[0;32m~/repos/LRATorch/lra-benchmarks-retnet/retnet.py:307\u001b[0m, in \u001b[0;36mGPTR.decode\u001b[0;34m(self, tokens, apply_positional_embedding)\u001b[0m\n\u001b[1;32m    305\u001b[0m     xe \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m pos\n\u001b[1;32m    306\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(xe)\n\u001b[0;32m--> 307\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(x)\n\u001b[1;32m    308\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/ddpm_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ddpm_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/LRATorch/lra-benchmarks-retnet/retnet.py:255\u001b[0m, in \u001b[0;36mRetentionEncoder.forward\u001b[0;34m(self, values)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, values : Float[Array, \u001b[39m\"\u001b[39m\u001b[39mbatch seq dim\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Float[Array, \u001b[39m\"\u001b[39m\u001b[39mbatch seq dim\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    254\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 255\u001b[0m         values \u001b[39m=\u001b[39m layer(values)\n\u001b[1;32m    256\u001b[0m     values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln(values)\n\u001b[1;32m    257\u001b[0m     \u001b[39mreturn\u001b[39;00m values\n",
      "File \u001b[0;32m~/miniconda3/envs/ddpm_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ddpm_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/LRATorch/lra-benchmarks-retnet/retnet.py:235\u001b[0m, in \u001b[0;36mRetBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x : Float[Array, \u001b[39m\"\u001b[39m\u001b[39mbatch seq dim\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Float[Array, \u001b[39m\"\u001b[39m\u001b[39mbatch seq dim\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    234\u001b[0m     \u001b[39m# y = self.msr(x) + x\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmsr(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln1(x))) \u001b[39m+\u001b[39m x\n\u001b[1;32m    236\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln2(y))) \u001b[39m+\u001b[39m y\n\u001b[1;32m    237\u001b[0m     \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/miniconda3/envs/ddpm_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ddpm_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/LRATorch/lra-benchmarks-retnet/retnet.py:159\u001b[0m, in \u001b[0;36mMSRBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x : Float[Array, \u001b[39m\"\u001b[39m\u001b[39mbatch seq dim\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Float[Array, \u001b[39m\"\u001b[39m\u001b[39mbatch seq dim\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    158\u001b[0m     \u001b[39m# return R\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     R \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparallel_retention(x)\n\u001b[1;32m    160\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_wg:\n\u001b[1;32m    161\u001b[0m         G \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(x\u001b[39m@self\u001b[39m\u001b[39m.\u001b[39mWG) \u001b[39m#[nbatch, nseq, model]\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/LRATorch/lra-benchmarks-retnet/retnet.py:149\u001b[0m, in \u001b[0;36mMSRBlock.parallel_retention\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    147\u001b[0m QK \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m'\u001b[39m\u001b[39mbihd, bjhd -> bhij\u001b[39m\u001b[39m'\u001b[39m, Q, K) \u001b[39m#(nbatch, nheads, nseq, nseq)\u001b[39;00m\n\u001b[1;32m    148\u001b[0m D \u001b[39m=\u001b[39m make_scale_matrix(nseq, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale) \u001b[39m#(nheads, nseq, nseq)\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m QKD \u001b[39m=\u001b[39m QK\u001b[39m*\u001b[39;49mD \u001b[39m#(nbatch, nheads, nseq, nseq)\u001b[39;00m\n\u001b[1;32m    150\u001b[0m V \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m'\u001b[39m\u001b[39mbsd, hud -> bshu\u001b[39m\u001b[39m'\u001b[39m, x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mWV) \u001b[39m#[nbatch, nseq, nheads, dhead]\u001b[39;00m\n\u001b[1;32m    151\u001b[0m R \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m'\u001b[39m\u001b[39mbhij, bjhd -> bihd\u001b[39m\u001b[39m'\u001b[39m, QKD, V) \u001b[39m#[batch, nheads, nseq, dhead]\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.80 GiB. GPU 0 has a total capacty of 15.77 GiB of which 333.12 MiB is free. Including non-PyTorch memory, this process has 15.44 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 837.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer = lightning.Trainer(max_epochs=2, accumulate_grad_batches=8)\n",
    "trainer.fit(model=module, train_dataloaders=train_dataloader, val_dataloaders=valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
